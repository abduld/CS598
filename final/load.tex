\subsection{Processor Utilization}


\begin{figure*}
  \centering

  \begin{subfigure}[b]{\textwidth}
          \centering
          \includegraphics[width=0.3\textwidth]{data/load_legend.pdf}
  \end{subfigure}

  \begin{subfigure}[b]{0.85\textwidth}
       \centering
       \includegraphics[width=\textwidth]{data/load_vectoradd_nexus5.pdf}
       \vspace{-5mm}
       \caption{VectorAdd on Nexus 5}\label{fig:Vecadd5}
   \end{subfigure}

  \begin{subfigure}[b]{0.85\textwidth}
       \centering
       \includegraphics[width=\textwidth]{data/load_vectoradd_nexus7.pdf}
       \vspace{-5mm}
       \caption{VectorAdd on Nexus 7}\label{fig:Vecadd7}
   \end{subfigure}

  \begin{subfigure}[b]{0.85\textwidth}
       \centering
       \includegraphics[width=\textwidth]{data/load_sgemm_nexus5.pdf}
       \vspace{-5mm}
       \caption{Sgemm on Nexus 5}\label{fig:Sgemm5}
   \end{subfigure}
  \begin{subfigure}[b]{0.85\textwidth}
       \centering
       \includegraphics[width=\textwidth]{data/load_sgemm_nexus7.pdf}
       \vspace{-5mm}
       \caption{Sgemm on Nexus 7}\label{fig:Sgemm7}
   \end{subfigure}


  \caption{Processor utilization of VectorAdd and SGEMM for both Nexus 5 and Nexus 7. The $x$ axis is time, and the $y$ axis is normalized to the peek utilization for CPUs or GPUs across implementations.}
  \label{fig:loadVecAddSgemm}
\end{figure*}

Mobile devices use dynamic voltage frequency scaling (DVFS)
  to match performance to power utilization.
The frequency is increased for the processors when the load goes over a certain
  threshold.
Both CPUs and GPUs make use of frequency scaling. But unlike CPUs (which typically
  have very fine-grained frequencies --- operating at around 10 different frequencies), GPUs
  have coarse-grained frequencies (operating at only 4 different frequencies).  

Processor utilization is measured by multiplying the processor
  load and frequency information collected via Trepn.
The data is measured while the system is connected to a
  power source, which avoids the device going to sleep (it does add the
  extra overhead of the device communicating debug information with the development
  machine).
Since Trepn has a $100ms$ measure granularity, the kernel code is run $100$ times
  ($5$ times for MRIQ and TPACF), this allows us to visualize the trend in resource
  utilization.
The information gives us insight as to what parts of the code are active in each
	section of the code, how that impacts performance and battery.


As per standard Android applications, the UI runs in a separate UI thread 
  independent from the computation.
Trepn, which has a high overhead ($10\%$ on average), also runs as a background
  process.
These background processes are echoed in
  the plots --- a single threaded Java code, for example, should not utilize more than one core, but our plots show that more than one core is active.


The VectorAdd benchmark has a very high memory to compute ratio, therefore the processor utilization
  plot~(figure \ref{fig:loadVecAddSgemm}) shows that not all CPUs 
  were fully utilized during the compute phase.
For the Nexus 5 device, while the graphs show a GPU was being used during the OpenMP,
  RenderScript, and Java, this is an error in the measurement --- we believe this is 
  due to the UI thread utilizing the GPU or some other interferences.
As expected, the GPU is not being utilized on the Nexus 7 except for OpenCL.

As discussed in the benchmark section, the Android GCC compiler was not able 
  to interpret the OpenMP \fix{pragma} code and therefore SGEMM ran in a single thread
  on both the Nexus 5 and Nexus 7 (figure~\ref{fig:loadVecAddSgemm}).
While OpenCL does utilize the GPU, because of the size of the matrix, the GPU has
  lower occupancy and therefore does not achieve peek performance.
Both the RenderScript and ThreadedJava code make full utilization of all the cores.

MRIQ is embarrassingly parallel, and we can see full utilization for both the CPUs and GPUs in figure~\ref{fig:loadMRIQTpacf}.
Abrupt dips in the plots show positions where a kernel launch occurs.
It is also interesting to note that the CPU on the Nexus 5 was not fully utilized for both ThreadedJava and RenderScript.
This is due to either the load being low for a high frequency or the frequency being 
  chosen to be low.

\begin{figure*}
  \centering

  \begin{subfigure}[b]{\textwidth}
          \centering
          \includegraphics[width=0.4\textwidth]{data/load_legend.pdf}
  \end{subfigure}

  \begin{subfigure}[b]{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{data/load_mriq_nexus5.pdf}
      \caption{MRIQ on Nexus 5}
      \label{fig:MRIQ5}
  \end{subfigure}
  \begin{subfigure}[b]{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{data/load_mriq_nexus7.pdf}
      \caption{MRIQ on Nexus 7}
      \label{fig:MRIQ7}
  \end{subfigure}

  \begin{subfigure}[b]{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{data/load_tpacf_nexus5.pdf}
      \caption{TPACF on Nexus 5}
      \label{fig:TPACF5}
  \end{subfigure}
  \begin{subfigure}[b]{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{data/load_tpacf_nexus7.pdf}
      \caption{TPACF on Nexus 7}
      \label{fig:TPACF7}
  \end{subfigure}

  \caption{Processor utilization of MRIQ and TPACF for both Nexus 5 and Nexus 7. The $x$ axis is time, and the $y$ axis is normalized to the peek utilization for CPUs or GPUs across implementations.}
  \label{fig:loadMRIQTpacf}
\end{figure*}

The TPACF code is divided into two parts.
The first is serial allocation and populating data
  that, for the purpose of code reuse, is done on the 
  Java side using a single thread.
Once that completes, we then execute the code on different datasets in parallel.
The utilization plots (figure~\ref{fig:loadMRIQTpacf}) show this behavior.  

For Histogram (figure~\ref{fig:loadHistogramStencil}), we see that memory starts
  to play a major role in processor utilization.
The GPU was being utilized in the memory copy as it was in the computation, for example.

For Stencil (figure~\ref{fig:loadHistogramStencil}), we see that RenderScript did
  not fully utilize all available CPU cores.
This is because the stencil kernel is memory access bound, performing around 10 flops 
  (ignoring index calculations).
We again see that memory copy (\fix{SetupArgs} for RenderScript and
  \fix{CopyTo} for OpenCL) resulted in a substantial amount of resources being
  wasted.

\begin{figure*}
  \centering

  \begin{subfigure}[b]{\textwidth}
          \centering
          \includegraphics[width=0.4\textwidth]{data/load_legend.pdf}
  \end{subfigure}

  \begin{subfigure}[b]{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{data/load_histogram_nexus5.pdf}
      \caption{Histogram on Nexus 5}
      \label{fig:Histogram5}
  \end{subfigure}
  \begin{subfigure}[b]{\textwidth}
      \centering
      \includegraphics[width=\textwidth]{data/load_stencil_nexus5.pdf}
      \caption{Stencil on Nexus 5}
      \label{fig:Stencil5}
  \end{subfigure}

  \caption{Processor utilization of Histogram and Stencil for both Nexus 5. The $x$ axis is time, and the $y$ axis is normalized to the peek utilization for CPUs or GPUs across implementations.}
  \label{fig:loadHistogramStencil}
\end{figure*}

Throughout the benchmarks, we see that RenderScript
	did not utilize the GPUs on the tested devices.
This is due to RenderScript requiring full
	IEEE 754-2008 compliance, which the GPUs are not able to provide.
Inserting the \fix{\#pragma rs\_fp\_imprecise} pragma into the
	RenderScript kernel allows for relaxed IEEE compliance and 
	should allow for GPU acceleration.
Similar options are available in OpenCL and C, but since
	this option is not available in Java, we chose to disable it. 
Further investigations would require us to analyze the results with relaxed precision.

